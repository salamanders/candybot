https://letsencrypt.org/docs/certificates-for-localhost/
https://support.google.com/pixelphone/answer/2844832?hl=en


keytool -keystore keystore.jks -alias ev3dev -genkeypair -keyalg RSA -keysize 4096 -validity 90 -dname 'CN=ev3dev, OU=ktor, O=ktor, L=Unspecified, ST=Unspecified, C=US'
pw: "changeit"

/usr/bin/keytool
package info.benjaminhill.candybot.plugins

import io.ktor.server.application.*
import io.ktor.server.response.*
import io.ktor.server.routing.*
import io.ktor.server.sessions.*
import kotlin.collections.set

fun Application.configureSecurity() {
    data class MySession(val count: Int = 0)
    install(Sessions) {
        cookie<MySession>("MY_SESSION") {
            cookie.extensions["SameSite"] = "lax"
        }
    }

    routing {
        get("/session/increment") {
            val session = call.sessions.get() ?: MySession()
            call.sessions.set(session.copy(count = session.count + 1))
            call.respondText("Counter is ${session.count}. Refresh to increment.")
        }
    }
}

<label>
    <input type="range" min="-100" max="100" step="1.0">
</label>


How to use movenet/singlepose/lightning/tflite/int8
    <!--
    <script src="https://unpkg.com/@tensorflow/tfjs"></script>
    <script src="https://unpkg.com/@tensorflow-models/posenet"></script>
    -->

    /*
    context.drawImage(video, 0, 0, 640, 480);
    const imageScaleFactor = 0.50;
    const flipHorizontal = false;
    const outputStride = 16;
    const imageElement = document.getElementById('canvas');
    console.time("estimateSinglePose");
    const pose = await net.estimateSinglePose(imageElement, imageScaleFactor, flipHorizontal, outputStride);
    console.timeEnd("estimateSinglePose");
    console.info(pose);
     */
 // https://github.com/tensorflow/tfjs-models/tree/master/pose-detection/src/movenet
 //import * as poseDetection from 'https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection';
 //
 //import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl';
 // import '@tensorflow/tfjs-backend-wasm';



// TODO: tf.loadGraphModel("https://tfhub.dev/google/movenet/singlepose/lightning/tfjs/4", { fromTFHub: true });
// TODO: https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4
// TODO https://tfhub.dev/google/movenet/singlepose/lightning/4
// TODO: https://blog.tensorflow.org/2018/05/real-time-human-pose-estimation-in.html

//const net = await posenet.load();
// Elements for taking the snapshot


  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>

async function detectPoses() {
  const model = poseDetection.SupportedModels.BlazePose;
  const detectorConfig = {
    runtime: 'mediapipe',
    modelType: 'lite'
  };
  detector = await poseDetection.createDetector(model, detectorConfig);

  const video = document.getElementById('video');

  const estimationConfig = {enableSmoothing: true};
  const poses = await detector.estimatePoses(video, estimationConfig);
}

detectPoses();





<html>
<body>
<!-- Load TensorFlow.js -->
<script src="https://unpkg.com/@tensorflow/tfjs"></script>
<!-- Load Posenet -->
<script src="https://unpkg.com/@tensorflow-models/posenet">
</script>
<script type="text/javascript">
    posenet.load().then(function(net) {
        // posenet model loaded
    });
</script>
</body>
</html>



<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
<!-- Include below scripts if you want to use TF.js runtime. -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>

<!-- Optional: Include below scripts if you want to use MediaPipe runtime. -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>


const model = poseDetection.SupportedModels.BlazePose;
const detectorConfig = {
  runtime: 'mediapipe', // or 'tfjs'
  modelType: 'lite'
};
detector = await poseDetection.createDetector(model, detectorConfig);

const video = document.getElementById('video');
const poses = await detector.estimatePoses(video);


// https://blog.tensorflow.org/2022/01/body-segmentation.html
// https://blog.tensorflow.org/2021/11/3D-handpose.html
// https://github.com/tensorflow/tfjs-models/tree/master/pose-detection/src/blazepose_mediapipe

<!-- Require the peer dependencies of pose-detection. -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>

<!-- You must explicitly require a TF.js backend if you're not using the TF.js union bundle. -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>

const model = poseDetection.SupportedModels.BlazePose;
const detectorConfig = {
  runtime: 'mediapipe',
  solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/pose'
                // or 'base/node_modules/@mediapipe/pose' in npm.
};
detector = await poseDetection.createDetector(model, detectorConfig);

const estimationConfig = {enableSmoothing: true};
const poses = await detector.estimatePoses(image, estimationConfig);